#!/usr/bin/env python3
"""
Parse cleaned Nazarzoda Tajik Dictionary text into structured data
VERSION 3 - FINAL with Arabic word order fix
"""

import pandas as pd
import re
import os
from datetime import datetime

# Set paths
hdir = os.path.expanduser('~')
project_dir = os.path.join(hdir, 'Projects/persian-dictionary/nazarzoda_tajik_dictionary_parser')
output_dir = os.path.join(hdir, 'Dropbox/Active_Directories/Inbox')

# Input file
input_file = os.path.join(project_dir, 'nazarzoda_full_cleaned_20251222_123804.txt')


# ABBREVIATION EXPANSIONS
# Language/Etymology markers (appear BEFORE Arabic)
LANGUAGE_ETYMOLOGY = {
    'Ğ°.': 'Ğ°Ñ€Ğ°Ğ±Ó£',
    'Ğ°Ğ½Ğ³Ğ».': 'Ğ°Ğ½Ğ³Ğ»Ğ¸ÑÓ£',
    'Ğ°Ğº.': 'Ğ°ĞºĞºĞ°Ğ´Ó£',
    'Ğ¸Ğ±Ñ€.': 'Ğ¸Ğ±Ñ€Ó£',
    'Ğ¸ÑĞ¿.': 'Ğ¸ÑĞ¿Ğ°Ğ½Ó£',
    'Ğ¸Ñ‚.': 'Ğ¸Ñ‚Ğ°Ğ»Ğ¸ÑĞ²Ó£',
    'ĞºĞ¸Ñ‚.': 'ĞºĞ¸Ñ‚Ğ¾Ğ±Ó£',
    'Ğ»Ğ¾Ñ‚.': 'Ğ»Ğ¾Ñ‚Ğ¸Ğ½Ó£',
    'Ğ¼.': 'Ğ¼ÑƒÒ“ÑƒĞ»Ó£',
    'Ğ¼Ğ°Ğ».': 'Ğ¼Ğ°Ğ»Ğ°Ğ¹Ğ·Ó£',
    'Ğ¾Ğ»Ğ¼.': 'Ğ¾Ğ»Ğ¼Ğ¾Ğ½Ó£',
    'Ğ¿Ğ¾Ğ».': 'Ğ¿Ğ¾Ğ»Ğ°Ğ½Ğ´Ó£',
    'Ğ¿Ğ¾Ñ€Ñ‚.': 'Ğ¿Ğ¾Ñ€Ñ‚ÑƒĞ³Ğ°Ğ»Ó£',
    'Ñ€.': 'Ñ€ÑƒÑÓ£',
    'ÑĞ°Ğ½Ñ.': 'ÑĞ°Ğ½ÑĞºÑ€Ğ¸Ñ‚',
    'ÑÑƒÑ€.': 'ÑÑƒÑ€Ğ¸Ñ‘Ğ½Ó£',
    'Ñ‚.': 'Ñ‚ÑƒÑ€ĞºÓ£',
    'Ñ‚.-Ğ¼.': 'Ñ‚ÑƒÑ€ĞºĞ¸Ñ Ğ¼ÑƒÒ“ÑƒĞ»Ó£',
    'Ñ‚Ğ¸Ğ±ĞµÑ‚.': 'Ñ‚Ğ¸Ğ±ĞµÑ‚Ó£',
    'Ñ„Ğ¸Ğ½.': 'Ñ„Ğ¸Ğ½Ğ»Ğ°Ğ½Ğ´Ó£',
    'Ñ„Ñ€.': 'Ñ„Ğ°Ñ€Ğ¾Ğ½ÑĞ°Ğ²Ó£',
    'Ñ…Ğ¸Ñ‚.': 'Ñ…Ğ¸Ñ‚Ğ¾Ó£',
    'Ñš.': 'Ò³Ğ¸Ğ½Ğ´Ó£',
    'ÑšĞ¾Ğ».': 'Ò³Ğ¾Ğ»Ğ»Ğ°Ğ½Ğ´Ó£',
    'Ñ‡.': 'Ñ‡ĞµÑ…Ó£',
    'ÑˆĞ²ĞµĞ´.': 'ÑˆĞ²ĞµĞ´Ó£',
    'Ñ.': 'ÑĞ½Ğ¾Ğ½Ó£',
    'Ñ.': 'ÑÒ³ÑƒĞ´Ó£',
    'ÑĞ¿.': 'ÑĞ¿Ğ¾Ğ½Ó£',
    'Ğ´.': 'Ğ´Ğ¸Ğ½Ó£',
}

# Register/Domain markers (appear AFTER Arabic)
REGISTER_DOMAIN = {
    'Ğ°Ğ´Ñˆ.': 'Ğ°Ğ´Ğ°Ğ±Ğ¸Ñ‘Ñ‚ÑˆĞ¸Ğ½Ğ¾ÑÓ£',
    'Ğ°Ğ½Ğ°Ñ‚.': 'Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ',
    'Ğ°ÑĞ¾Ñ‚.': 'Ğ°ÑĞ¾Ñ‚Ğ¸Ñ€Ó£, Ğ°ÑĞ¾Ñ‚Ğ¸Ñ€ÑˆĞ¸Ğ½Ğ¾ÑÓ£',
    'Ğ°Ñ„Ñ.': 'Ğ°Ñ„ÑĞ¾Ğ½Ğ°Ğ²Ó£',
    'Ğ±Ğ°Ğ¹Ñ‚.': 'Ğ±Ğ°Ğ¹Ñ‚Ğ¾Ñ€Ó£',
    'Ğ±Ğ°Ñ€Ò›.': 'Ğ±Ğ°Ñ€Ò›Ó£, ÑĞ»ĞµĞºÑ‚Ñ€Ó£',
    'Ğ±Ğ°Ò³Ñ€.': 'Ğ±Ğ°Ò³Ñ€Ğ½Ğ°Ğ²Ğ°Ñ€Ğ´Ó£',
    'Ğ±Ğ¸Ğ¾Ğ».': 'Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ',
    'Ğ±Ğ¾Ò“Ğ¿.': 'Ğ±Ğ¾Ò“Ğ¿Ğ°Ñ€Ğ²Ğ°Ñ€Ó£',
    'Ğ±Ğ¾Ñ‚.': 'Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¸ĞºĞ°',
    'Ğ±Ğ¾Ñ„.': 'Ğ±Ğ¾Ñ„Ğ°Ğ½Ğ´Ğ°Ğ³Ó£',
    'Ğ²Ğ°Ñ€Ğ·.': 'Ğ²Ğ°Ñ€Ğ·Ğ¸Ñˆ',
    'Ğ³ĞµĞ¾Ğ».': 'Ğ³ĞµĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ',
    'Ğ³Ñ€Ğ°Ğ¼.': 'Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ°',
    'Ğ³ÑƒÑ„Ñ‚.': 'Ğ³ÑƒÑ„Ñ‚ÑƒĞ³Ó¯Ó£',
    'Ğ´Ó¯Ğ·.': 'Ğ´Ó¯Ğ·Ğ°Ğ½Ğ´Ğ°Ğ³Ó£',
    'Ğ·Ğ±Ñˆ.': 'Ğ·Ğ°Ğ±Ğ¾Ğ½ÑˆĞ¸Ğ½Ğ¾ÑÓ£',
    'Ğ·Ğ¾Ğ¾Ğ».': 'Ğ·Ğ¾Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ',
    'Ğ¸Ò›Ñ‚.': 'Ğ¸Ò›Ñ‚Ğ¸ÑĞ¾Ğ´',
    'Ğ¸Ñ‚Ñ‚.': 'Ğ¸Ñ‚Ñ‚Ğ¸Ğ»Ğ¾Ğ¾Ñ‚ÑˆĞ¸Ğ½Ğ¾ÑÓ£',
    'ĞºĞ°Ğ¹Ò³.': 'ĞºĞ°Ğ¹Ò³Ğ¾Ğ½Ğ½Ğ°Ğ²Ğ°Ñ€Ğ´Ó£',
    'ĞºĞ¸Ğ½.': 'ĞºĞ¸Ğ½Ğ¾ÑĞ²Ó£',
    'ĞºĞ¸Ñ‚.': 'ĞºĞ¸Ñ‚Ğ¾Ğ±Ó£',
    'ĞºĞ¸ÑˆĞ¾Ğ².': 'ĞºĞ¸ÑˆĞ¾Ğ²Ğ°Ñ€Ğ·Ó£',
    'ĞºÒ³Ğ½.': 'ĞºÓ¯Ò³Ğ½Ğ°ÑˆÑƒĞ´Ğ°',
    'Ğ»Ğ°Ò³Ò·.': 'Ğ»Ğ°Ò³Ò·Ğ°Ğ²Ó£',
    'Ğ¼Ğ°Ğ½Ñ‚.': 'Ğ¼Ğ°Ğ½Ñ‚Ğ¸Ò›',
    'Ğ¼Ğ°Ò·.': 'Ğ¼Ğ°Ò·Ğ¾Ğ·Ğ°Ğ½',
    'Ğ¼Ğ°ÑŠĞ´.': 'Ğ¼Ğ°ÑŠĞ´Ğ°Ğ½ÑˆĞ¸Ğ½Ğ¾ÑÓ£',
    'Ğ¼ĞµÑŠ.': 'Ğ¼ĞµÑŠĞ¼Ğ¾Ñ€Ó£',
    'Ğ¼Ğ¾Ğ».': 'Ğ¼Ğ¾Ğ»Ğ¸Ñ',
    'Ğ¼ÑƒÑ.': 'Ğ¼ÑƒÑĞ¸Ò›Ó£',
    'Ğ½Ğ°Ğ².': 'Ğ½Ğ°Ğ²ÑĞ¾Ñ…Ñ‚',
    'Ğ½Ğ°ÑˆÑ€.': 'Ğ½Ğ°ÑˆÑ€Ğ¸Ñ‘Ñ‚',
    'Ğ½ÑƒÒ·.': 'Ğ¸Ğ»Ğ¼Ğ¸ Ğ½ÑƒÒ·ÑƒĞ¼',
    'Ğ¾Ğ±Ò³Ñˆ.': 'Ğ¾Ğ±ÑƒÒ³Ğ°Ğ²Ğ¾ÑˆĞ¸Ğ½Ğ¾ÑÓ£',
    'Ğ¾Ğ¼Ó¯Ğ·.': 'Ğ¾Ğ¼Ó¯Ğ·Ğ³Ğ¾Ñ€Ó£',
    'Ñ€Ğ°Ğ´Ğ¸Ğ¾.': 'Ñ€Ğ°Ğ´Ğ¸Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°',
    'Ñ€Ğ¸Ñ‘Ğ·.': 'Ñ€Ğ¸Ñ‘Ğ·Ğ¸Ñ‘Ñ‚, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°',
    'Ñ€.-Ğ¾Ò³.': 'Ñ€Ğ¾Ò³Ğ¸ Ğ¾Ò³Ğ°Ğ½',
    'Ñ.': 'ÑĞ¸Ñ‘ÑÓ£',
    'ÑĞ°Ğ½ÑŠ.': 'ÑĞ°Ğ½ÑŠĞ°Ñ‚',
    'ÑĞ¾Ñ…Ñ‚.': 'ÑĞ¾Ñ…Ñ‚Ğ¼Ğ¾Ğ½',
    'Ñ‚Ğ°Ò³Ò›.': 'ÑÑƒÑ…Ğ°Ğ½Ğ¸ Ñ‚Ğ°Ò³Ò›Ğ¸Ñ€Ğ¾Ğ¼ĞµĞ·',
    'Ñ‚Ğ°ÑŠÑ€.': 'Ñ‚Ğ°ÑŠÑ€Ğ¸Ñ…Ó£',
    'Ñ‚ĞµÑ….': 'Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°',
    'Ñ‚Ğ¸Ğ±.': 'Ñ‚Ğ¸Ğ±Ğ±Ó£',
    'Ñ„Ğ°Ğ»Ñ.': 'Ñ„Ğ°Ğ»ÑĞ°Ñ„Ğ°',
    'Ñ„Ğ¸Ğ·.': 'Ñ„Ğ¸Ğ·Ğ¸ĞºĞ°',
    'Ñ„Ğ¾Ğ»Ğº.': 'Ñ„Ğ¾Ğ»ĞºĞ»Ğ¾Ñ€',
    'Ñ…Ğ¸Ğ¼.': 'Ñ…Ğ¸Ğ¼Ğ¸Ñ',
    'Ñ…Ó¯Ñ€.': 'Ñ…Ó¯Ñ€Ğ¾ĞºĞ²Ğ¾Ñ€Ó£',
    'Ñš.': 'Ò³Ğ°Ñ€Ğ±Ó£',
    'Ò³Ğ°Ğ½Ğ´.': 'Ò³Ğ°Ğ½Ğ´Ğ°ÑĞ°',
    'Ò³Ğ¸ÑĞ¾Ğ±Ğ´.': 'Ò³Ğ¸ÑĞ¾Ğ±Ğ´Ğ¾Ñ€Ó£',
    'Ò³ÑƒÒ›.': 'Ò³ÑƒÒ›ÑƒÒ›ÑˆĞ¸Ğ½Ğ¾ÑÓ£',
    'Ñ‡Ğ¾Ñ€Ğ².': 'Ñ‡Ğ¾Ñ€Ğ²Ğ¾Ğ´Ğ¾Ñ€Ó£',
    'Ò·ÑƒÒ“Ñ€.': 'Ò·ÑƒÒ“Ñ€Ğ¾Ñ„Ğ¸Ñ',
}


def _reverse_arabic_word_order(arabic_text):
    """
    Reverse word order in multi-word Arabic phrases.
    
    Arabic is RTL, so when we extract "word1 word2" from PDF,
    it's actually already reversed. We need to flip the word order
    back to logical order.
    
    Examples:
        "Ø¨Ù‡Ø§ Ø¢Ø¨" â†’ "Ø¢Ø¨ Ø¨Ù‡Ø§"  (correct: Äb bahÄ = water price)
        "Ù…Ú¯ÙˆÙ† Ø§Ø¨Ø±ÛŒØ´" â†’ "Ø§Ø¨Ø±ÛŒØ´ Ù…Ú¯ÙˆÙ†"  (correct: abrÄ“shim-gÅ«n)
    
    Args:
        arabic_text: Arabic script text (may contain spaces)
    
    Returns:
        Text with word order reversed (if multiple words)
    """
    if not arabic_text:
        return arabic_text
    
    # Split on whitespace
    words = arabic_text.split()
    
    # If only one word, no reversal needed
    if len(words) == 1:
        return arabic_text
    
    # Reverse word order
    reversed_words = words[::-1]
    
    return ' '.join(reversed_words)


def parse_dictionary_text(text):
    """
    Parse extracted dictionary text into structured DataFrame.
    
    FIXED: Stricter entry boundary detection to prevent cross-contamination
    FIXED: Arabic word order reversal for multi-word phrases
    
    Args:
        text: Full extracted text from PDF
    
    Returns:
        DataFrame with columns: headword, arabic, language_marker, register, 
                               definition_number, definition
    """
    
    print("\n" + "="*70)
    print("ğŸ“– PARSING DICTIONARY ENTRIES (VERSION 3 - FINAL)")
    print("="*70)
    
    # Remove page markers for cleaner parsing
    text = re.sub(r'--- PAGE \d+ ---\n', '', text)
    
    entries = []
    
    # Split into lines for processing
    lines = text.split('\n')
    
    current_entry = None
    current_lines = []
    
    print("ğŸ” Identifying entries with stricter boundary detection...")
    entry_count = 0
    skipped_false_positives = 0
    
    for line_num, line in enumerate(lines):
        # Stricter pattern - requires Arabic script within next 100 chars
        header_match = re.match(r'^\s*([Ğ-Ğ¯ĞÓ¢ÒšÒ’Ò²ĞÒ¶]{2,}(?:\s+[IVX]+|//[Ğ-Ğ¯ĞÓ¢ÒšÒ’Ò²ĞÒ¶]+)*)\s+', line)
        
        if header_match:
            # Validate that Arabic script follows soon after headword
            # This prevents false positives from inline uppercase text
            check_position = header_match.end()
            check_text = line[check_position:check_position + 100]
            
            # Require Arabic script OR specific markers (like abbreviations)
            has_arabic = bool(re.search(r'[\u0600-\u06FF]', check_text))
            has_marker = bool(re.match(r'^([Ğ°-ÑÑ‘Ó£Ò›Ò“Ò³ÑÒ·][Ğ°-ÑÑ‘Ó£Ò›Ò“Ò³ÑÒ·.-]{0,6})\s+', check_text))
            
            if not (has_arabic or has_marker):
                # False positive - this is not a new entry
                # It's likely uppercase text within a definition
                skipped_false_positives += 1
                if current_entry is not None:
                    current_lines.append(line.strip())
                continue
            
            # Save previous entry if exists
            if current_entry is not None:
                entry_data = _process_entry(current_entry, '\n'.join(current_lines))
                if entry_data:
                    entries.extend(entry_data)
                    entry_count += 1
            
            # Start new entry
            current_entry = line.strip()
            current_lines = []
            
            # Progress indicator
            if entry_count > 0 and entry_count % 1000 == 0:
                print(f"   Processed {entry_count:,} entries... (skipped {skipped_false_positives:,} false positives)")
        else:
            # Continuation of current entry
            if current_entry is not None:
                current_lines.append(line.strip())
    
    # Don't forget last entry
    if current_entry is not None:
        entry_data = _process_entry(current_entry, '\n'.join(current_lines))
        if entry_data:
            entries.extend(entry_data)
            entry_count += 1
    
    print(f"âœ… Identified {entry_count:,} unique headwords")
    print(f"   Skipped {skipped_false_positives:,} false positives (inline uppercase text)")
    print(f"âœ… Generated {len(entries):,} total rows (including sub-definitions)")
    
    # Create DataFrame
    df = pd.DataFrame(entries)
    
    return df


def _process_entry(header_line, definition_text):
    """
    Parse a single dictionary entry into structured data.
    
    EXPANDED: Now expands abbreviations based on position
    - language_marker: expanded full form (e.g., Ğ°. â†’ Ğ°Ñ€Ğ°Ğ±Ó£)
    - register: expanded full form (e.g., ĞºĞ¸Ñ‚. â†’ ĞºĞ¸Ñ‚Ğ¾Ğ±Ó£)
    
    FIXED: Arabic word order reversal for multi-word phrases
    
    Args:
        header_line: First line with headword, Arabic, labels
        definition_text: Rest of entry (definitions, examples)
    
    Returns:
        List of dicts (one per definition if multiple definitions)
    """
    
    # Extract headword
    headword_match = re.match(r'^([Ğ-Ğ¯ĞÓ¢ÒšÒ’Ò²ĞÒ¶]{2,}(?:\s+[IVX]+|//[Ğ-Ğ¯ĞÓ¢ÒšÒ’Ò²ĞÒ¶]+)*)', header_line)
    if not headword_match:
        return None
    
    headword = headword_match.group(1).strip()
    
    # Build working string after headword
    remainder = header_line[headword_match.end():].strip()
    
    # Extract language marker (etymology) - comes BEFORE Arabic
    # Pattern: 1-4 lowercase Cyrillic letters + period, followed by space and Arabic
    language_marker = None
    language_marker_abbrev = None
    lang_match = re.match(r'^([Ğ°-ÑÑ‘Ó£Ò›Ò“Ò³ÑÒ·][Ğ°-ÑÑ‘Ó£Ò›Ò“Ò³ÑÒ·.-]{0,6})\s+(?=[\u0600-\u06FF])', remainder)
    if lang_match:
        language_marker_abbrev = lang_match.group(1)
        # Expand abbreviation
        language_marker = LANGUAGE_ETYMOLOGY.get(language_marker_abbrev, language_marker_abbrev)
        remainder = remainder[lang_match.end():].strip()
    
    # Extract Arabic script - captures full phrase including spaces
    arabic = None
    arabic_match = re.match(r'^([\u0600-\u06FF]+(?:\s+[\u0600-\u06FF]+)*)', remainder)
    if arabic_match:
        arabic = arabic_match.group(1).strip()
        
        # âœ… FIX: Reverse word order for multi-word Arabic phrases
        arabic = _reverse_arabic_word_order(arabic)
        
        remainder = remainder[arabic_match.end():].strip()
    
    # Extract register marker - comes AFTER Arabic
    # Search for any register marker in REGISTER_DOMAIN keys
    register = None
    register_abbrev = None
    
    # Sort by length (longest first) to match 'Ñ‚.-Ğ¼.' before 'Ñ‚.'
    register_markers = sorted(REGISTER_DOMAIN.keys(), key=len, reverse=True)
    
    for marker in register_markers:
        # Look for marker followed by space or at start of remainder
        if remainder.startswith(marker + ' ') or remainder == marker:
            register_abbrev = marker
            register = REGISTER_DOMAIN[marker]
            remainder = remainder[len(marker):].strip()
            break
    
    # Everything remaining is definition (including Ğ½Ğ¸Ğ³., Ğ¼Ğ°Ğ½ÑÑƒĞ± Ğ±Ğ°, etc.)
    # Parse numbered definitions
    full_text = remainder + '\n' + definition_text
    definitions = _parse_definitions(full_text)
    
    # Clean definitions to remove cross-contamination
    # If a definition contains a new entry pattern, truncate it
    definitions = _clean_definitions(definitions)
    
    # Create base entry
    base_entry = {
        'headword': headword,
        'arabic': arabic,
        'language_marker': language_marker,  # Expanded form
        'register': register,  # Expanded form
    }
    
    # If multiple numbered definitions, create one row per definition
    if definitions:
        result = []
        for def_num, def_text in definitions:
            entry = base_entry.copy()
            entry['definition_number'] = def_num
            entry['definition'] = def_text.strip()
            result.append(entry)
        return result
    else:
        # Single definition (no numbering)
        definition = remainder.strip()
        
        # If definition is empty, use definition_text
        if not definition:
            definition = definition_text.strip()
        
        # Clean single definition too
        definition = _clean_single_definition(definition)
        
        base_entry['definition_number'] = None
        base_entry['definition'] = definition
        return [base_entry]


def _parse_definitions(text):
    """
    Parse numbered definitions (1., 2., 3., etc.) from entry text.
    
    Args:
        text: Full entry text
    
    Returns:
        List of (number, definition_text) tuples
    """
    
    # Find all numbered definitions
    pattern = r'(\d+)\.\s+([^0-9]+?)(?=\s+\d+\.|$)'
    matches = re.findall(pattern, text, re.DOTALL)
    
    if matches:
        # Clean up each definition
        cleaned_matches = []
        for num, defn in matches:
            # Replace multiple whitespaces/newlines with single space
            cleaned_defn = re.sub(r'\s+', ' ', defn.strip())
            cleaned_matches.append((int(num), cleaned_defn))
        return cleaned_matches
    else:
        return []


def _clean_definitions(definitions):
    """
    Remove cross-contamination from numbered definitions.
    
    Truncates definitions at the first sign of a new entry (uppercase + Arabic).
    
    Args:
        definitions: List of (number, text) tuples
    
    Returns:
        Cleaned list of (number, text) tuples
    """
    contamination_pattern = r'([Ğ-Ğ¯ĞÓ¢ÒšÒ’Ò²ĞÒ¶]{2,})\s+([\u0600-\u06FF]+)'
    
    cleaned = []
    for num, text in definitions:
        # Find first occurrence of contamination pattern
        match = re.search(contamination_pattern, text)
        if match:
            # Truncate at this point
            text = text[:match.start()].strip()
        
        cleaned.append((num, text))
    
    return cleaned


def _clean_single_definition(text):
    """
    Remove cross-contamination from single definition.
    
    Args:
        text: Definition text
    
    Returns:
        Cleaned text
    """
    contamination_pattern = r'([Ğ-Ğ¯ĞÓ¢ÒšÒ’Ò²ĞÒ¶]{2,})\s+([\u0600-\u06FF]+)'
    
    match = re.search(contamination_pattern, text)
    if match:
        # Truncate at this point
        text = text[:match.start()].strip()
    
    return text


def analyze_dataframe(df):
    """
    Generate detailed statistics and quality report for parsed data.
    """
    
    print("\n" + "="*70)
    print("ğŸ“Š PARSING STATISTICS & QUALITY REPORT")
    print("="*70)
    
    # Basic counts
    print(f"\nğŸ“ˆ Basic Statistics:")
    print(f"   Total rows: {len(df):,}")
    print(f"   Unique headwords: {df['headword'].nunique():,}")
    print(f"   Entries with Arabic script: {df['arabic'].notna().sum():,} ({df['arabic'].notna().sum()/len(df)*100:.1f}%)")
    print(f"   Entries with language markers: {df['language_marker'].notna().sum():,} ({df['language_marker'].notna().sum()/len(df)*100:.1f}%)")
    print(f"   Entries with register markers: {df['register'].notna().sum():,} ({df['register'].notna().sum()/len(df)*100:.1f}%)")
    print(f"   Entries with numbered definitions: {df['definition_number'].notna().sum():,} ({df['definition_number'].notna().sum()/len(df)*100:.1f}%)")
    
    # Language marker distribution (EXPANDED FORMS)
    if df['language_marker'].notna().any():
        print(f"\nğŸŒ Language Marker Distribution (Etymology) - Top 15:")
        lang_counts = df['language_marker'].value_counts()
        for i, (marker, count) in enumerate(lang_counts.head(15).items(), 1):
            print(f"   {i:2d}. {marker:25s}: {count:>6,} entries ({count/len(df)*100:>5.1f}%)")
    
    # Register distribution (EXPANDED FORMS)
    if df['register'].notna().any():
        print(f"\nğŸ“š Register Marker Distribution - Top 15:")
        reg_counts = df['register'].value_counts()
        for i, (marker, count) in enumerate(reg_counts.head(15).items(), 1):
            print(f"   {i:2d}. {marker:25s}: {count:>6,} entries ({count/len(df)*100:>5.1f}%)")
    
    # Definition number distribution
    if df['definition_number'].notna().any():
        print(f"\nğŸ”¢ Definition Number Distribution:")
        def_counts = df['definition_number'].value_counts().sort_index()
        for def_num, count in def_counts.head(10).items():
            print(f"   Definition {int(def_num):2d}: {count:>6,} entries")


def show_sample_entries(df, n=15):
    """
    Display sample entries in a readable format.
    """
    
    print("\n" + "="*70)
    print(f"ğŸ“‹ SAMPLE ENTRIES (showing {n})")
    print("="*70)
    
    for i, row in df.head(n).iterrows():
        print(f"\n{i+1}. {row['headword']}")
        if pd.notna(row['arabic']):
            print(f"   Arabic: {row['arabic']}")
        if pd.notna(row['language_marker']):
            print(f"   Etymology: {row['language_marker']}")
        if pd.notna(row['register']):
            print(f"   Register: {row['register']}")
        if pd.notna(row['definition_number']):
            def_preview = row['definition'][:100] + "..." if len(row['definition']) > 100 else row['definition']
            print(f"   Def #{int(row['definition_number'])}: {def_preview}")
        else:
            def_preview = row['definition'][:100] + "..." if len(row['definition']) > 100 else row['definition']
            print(f"   Definition: {def_preview}")


def save_to_csv(df, output_path=None):
    """
    Save DataFrame to CSV with automatic naming.
    """
    
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(output_dir, f'nazarzoda_parsed_v3_final_{timestamp}.csv')
    
    print(f"\nğŸ’¾ Saving to CSV: {output_path}")
    df.to_csv(output_path, index=False, encoding='utf-8')
    print("âœ… Saved successfully!")
    
    return output_path


def parse_from_file(input_file, output_csv=None):
    """
    Complete parsing pipeline from file to CSV.
    """
    
    print("\n" + "="*70)
    print("ğŸ“š NAZARZODA DICTIONARY PARSER (VERSION 3 - FINAL)")
    print("="*70)
    print(f"ğŸ“„ Input: {input_file}")
    
    # Verify file exists
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"Input file not found: {input_file}")
    
    # Read text file
    print("\nğŸ“– Reading cleaned text file...")
    with open(input_file, 'r', encoding='utf-8') as f:
        text = f.read()
    
    print(f"   File size: {len(text):,} characters")
    print(f"   File lines: {len(text.splitlines()):,}")
    
    # Parse text
    df = parse_dictionary_text(text)
    
    # Analyze results
    analyze_dataframe(df)
    
    # Show samples
    show_sample_entries(df, n=15)
    
    # Save to CSV
    output_path = save_to_csv(df, output_csv)
    
    return df, output_path


# MAIN EXECUTION
if __name__ == "__main__":
    
    print("\n" + "="*70)
    print("ğŸ“š NAZARZODA TAJIK DICTIONARY PARSER - VERSION 3 FINAL")
    print("="*70)
    print("\nâœ¨ ALL IMPROVEMENTS:")
    print("   âœ… Stricter entry boundary detection (requires Arabic/markers)")
    print("   âœ… Cross-contamination cleaning (truncates at new entries)")
    print("   âœ… Added missing 'ĞºĞ¸Ñ‚.' abbreviation expansion")
    print("   âœ… Arabic word order reversal for multi-word phrases")
    
    try:
        df, output_path = parse_from_file(input_file)
        
        print("\n" + "="*70)
        print("âœ… PARSING COMPLETE")
        print("="*70)
        print(f"\nğŸ“ Output CSV: {output_path}")
        print(f"ğŸ“Š Total rows: {len(df):,}")
        print(f"ğŸ“– Unique headwords: {df['headword'].nunique():,}")
        
        print("\nğŸ’¡ Next steps:")
        print("   1. Check Arabic word order in output CSV")
        print("   2. Run diagnostics to verify improvements")
        print("   3. Import to SQLite")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()